import os
import re
import gc
import json
import fitz  # PyMuPDF
import easyocr
import pandas as pd
import torch
import whisper
from datetime import datetime
from konlpy.tag import Okt
from sentence_transformers import SentenceTransformer, util
from youtube_transcript_api import YouTubeTranscriptApi
from yt_dlp import YoutubeDL
from pathlib import Path
import logging
from supabase import create_client, Client

# [ÌôòÍ≤Ω ÏÑ§Ï†ï]
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
BASE_PATH = Path("/Volumes/Macbook_dat/Python/CoreWorkWord")
MODEL_PATH = BASE_PATH / "ai_models"
UPLOAD_PATH = BASE_PATH / "uploads"
RESULT_PATH = BASE_PATH / "results"

# [Supabase ÏÑ§Ï†ï] Î≥∏Ïù∏Ïùò Ï†ïÎ≥¥Î°ú ÍµêÏ≤¥ ÌïÑÏàò
SUPABASE_URL = "https://kggpojguiwnzmikibhzn.supabase.co"
SUPABASE_KEY = "sb_publishable_rFVuNo9uGALWqk5cUIEoAw_lXR2S2jH"
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

class CoreWorkCloudAnalyzer:
    def __init__(self):
        logging.info("üöÄ Korean Edu Factory v2.6 (DB-Ready) ÏóîÏßÑ Ï¥àÍ∏∞Ìôî...")
        self.device = "mps" if torch.backends.mps.is_available() else "cpu"
        
        # 1. AI Î™®Îç∏ Î°úÎìú
        self.embed_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS').to(self.device)
        self.reader = easyocr.Reader(['ko', 'en'], gpu=(self.device == "mps"), model_storage_directory=str(MODEL_PATH))
        self.stt_model = whisper.load_model("base", device=self.device, download_root=str(MODEL_PATH))
        self.okt = Okt()
        
        # 2. ÌÅ¥ÎùºÏö∞Îìú ÎßàÏä§ÌÑ∞ DB ÎèôÍ∏∞Ìôî (ÏûÑÎ≤†Îî© ÏÇ¨Ï†Ñ Í≥ÑÏÇ∞)
        self._sync_db_embeddings()

    def _sync_db_embeddings(self):
        """SupabaseÏóêÏÑú ÌôïÏ†ïÎêú Îã®Ïñ¥Îì§ÏùÑ Í∞ÄÏ†∏ÏôÄ Î≤°ÌÑ∞ÌôîÌïòÏó¨ GPU Î©îÎ™®Î¶¨Ïóê ÏÉÅÏ£º"""
        response = supabase.table("industry_keywords").select("*").execute()
        self.db_df = pd.DataFrame(response.data)
        
        # ÌôïÏ†ïÎêú(active) Îç∞Ïù¥ÌÑ∞Îßå Ï∂îÏ∂ú (Í≤ÄÌÜ†ÌïÑÏöî/ÎåÄÍ∏∞ Ï†úÏô∏)
        confirmed_df = self.db_df[self.db_df['status'] == 'active'].reset_index(drop=True)
        
        if not confirmed_df.empty:
            with torch.no_grad():
                self.db_embeddings = self.embed_model.encode(
                    confirmed_df['term'].tolist(), convert_to_tensor=True
                ).to(self.device)
            self.confirmed_df = confirmed_df
        else:
            self.db_embeddings = None

    def classify_semantic(self, nouns):
        """Tensor BatchÎ•º Ïù¥Ïö©Ìïú ÏùòÎØ∏ Í∏∞Î∞ò Î∂ÑÎ•ò (Threshold 0.65)"""
        if self.db_embeddings is None or not nouns:
            return ["ÏùºÎ∞ò/Í∏∞ÌÉÄ"] * len(nouns)
        
        results = []
        with torch.no_grad():
            input_embs = self.embed_model.encode(nouns, convert_to_tensor=True).to(self.device)
            cos_scores = util.cos_sim(input_embs, self.db_embeddings)
            top_scores, top_idxs = torch.max(cos_scores, dim=1)
            
            for i, score in enumerate(top_scores):
                if score > 0.65:
                    results.append(self.confirmed_df.iloc[top_idxs[i].item()]['sub_category'])
                else:
                    results.append("ÏùºÎ∞ò/Í∏∞ÌÉÄ")
        return results

    def upload_results(self, freq_df, source_info):
        """Î∂ÑÏÑù Í≤∞Í≥º Î∞è Ïã†Í∑ú Îã®Ïñ¥Î•º SupabaseÏóê ÏóÖÎ°úÎìú"""
        now = datetime.now().isoformat()
        
        # 1. ÌûàÏä§ÌÜ†Î¶¨ Í∏∞Î°ù (analysis_history ÌÖåÏù¥Î∏î)
        history_rows = []
        for _, row in freq_df.iterrows():
            history_rows.append({
                "source_name": source_info['file'],
                "word": row['Îã®Ïñ¥'],
                "frequency": int(row['ÎπàÎèÑ']),
                "category": row['Î∂ÑÎ•ò'],
                "analyzed_at": now
            })
        if history_rows:
            supabase.table("analysis_history").insert(history_rows).execute()

        # 2. Ïã†Í∑ú Îã®Ïñ¥ ÎßàÏä§ÌÑ∞ DB(industry_keywords) ÎåÄÍ∏∞Ïó¥ Îì±Î°ù
        unknowns = freq_df[freq_df['Î∂ÑÎ•ò'] == "ÏùºÎ∞ò/Í∏∞ÌÉÄ"]
        existing_terms = set(self.db_df['term'].tolist())
        new_entries = []
        
        for _, row in unknowns.iterrows():
            if row['Îã®Ïñ¥'] not in existing_terms:
                new_entries.append({
                    "term": row['Îã®Ïñ¥'],
                    "main_category": "ÎØ∏Î∂ÑÎ•ò_ÎåÄÍ∏∞",
                    "sub_category": "Î∂ÑÎ•òÌïÑÏöî_ÎåÄÍ∏∞",
                    "status": "candidate",
                    "source_type": source_info['type'],
                    "source_ref": source_info['file'],
                    "freq": int(row['ÎπàÎèÑ']),
                    "created_at": now,
                    "updated_at": now
                })
        
        if new_entries:
            supabase.table("industry_keywords").insert(new_entries).execute()
        logging.info(f"‚úÖ ÌÅ¥ÎùºÏö∞Îìú ÏóÖÎ°úÎìú ÏôÑÎ£å: {source_info['file']}")

    def analyze_file(self, file_path):
        """ÌååÏùºÎ≥Ñ Î∂ÑÏÑù Î©îÏù∏ ÌååÏù¥ÌîÑÎùºÏù∏"""
        ext = file_path.suffix.lower()
        if ext == '.pdf':
            doc = fitz.open(file_path); text = " ".join([p.get_text() for p in doc]); doc.close()
            s_type = "pdf"
        elif ext in ['.jpg', '.png', '.jpeg']:
            text = " ".join(self.reader.readtext(str(file_path), detail=0))
            s_type = "image"
        else: return

        nouns = [n for n in self.okt.nouns(re.sub(r'[^Í∞Ä-Ìû£\s]', '', text)) if len(n) > 1]
        if not nouns: return
        
        unique_nouns = list(set(nouns))
        classifications = self.classify_semantic(unique_nouns)
        mapping = dict(zip(unique_nouns, classifications))
        
        freq_df = pd.DataFrame(nouns, columns=['Îã®Ïñ¥']).value_counts().reset_index()
        freq_df.columns = ['Îã®Ïñ¥', 'ÎπàÎèÑ']
        freq_df['Î∂ÑÎ•ò'] = freq_df['Îã®Ïñ¥'].map(mapping)
        
        self.upload_results(freq_df, {"type": s_type, "file": file_path.name})

def main():
    analyzer = CoreWorkCloudAnalyzer()
    for f in UPLOAD_PATH.glob('*'):
        if f.suffix.lower() in ['.pdf', '.jpg', '.png']:
            logging.info(f"üìÑ Î∂ÑÏÑù Ï§ë: {f.name}")
            analyzer.analyze_file(f)
    if torch.backends.mps.is_available(): torch.mps.empty_cache()

if __name__ == "__main__":
    main()
